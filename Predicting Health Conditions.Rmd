---
title: "Predicting Health Conditions"
author: "David Stanko"
date: "2025-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Read in the datasets

```{r}
cancer.data <- read.csv("cancer_data.csv", stringsAsFactors = TRUE)
diabetes.data.linear <- read.csv("diabetes_data_linear.csv", stringsAsFactors = TRUE)
diabetes.data.nonlinear <- read.csv("diabetes_data_nonlinear.csv", stringsAsFactors = TRUE)
```
 
```{r}
head(cancer.data)
```

```{r}
head(diabetes.data.linear)
```

```{r}
head(diabetes.data.nonlinear)
```


## Split each dataset into a training and test set

```{r}
set.seed(1)

# We can use the number of rows of only one of the datasets, since they all have the same number of rows
test.indices <- sample(x=nrow(cancer.data), size = 0.25 * nrow(cancer.data), replace=FALSE)

cancer.train <- cancer.data[-test.indices,]
cancer.test <- cancer.data[test.indices,]

diabetes.linear.train <- diabetes.data.linear[-test.indices,]
diabetes.linear.test <- diabetes.data.linear[test.indices,]

diabetes.nonlinear.train <- diabetes.data.nonlinear[-test.indices,]
diabetes.nonlinear.test <- diabetes.data.nonlinear[test.indices,]
```

## Cost-Sensitive SVM: Why and How?

As we saw in `data_prep.ipynb`, the data has a severe class imbalance. When I ran SVM models without class weighting (which I will explain shortly), the model predicted every single training and test sample as the majority class. There was not even a decision boundary because everything was being predicted as one class. Without a decision boundary, our results won't mean anything.

The solution is cost-sensitive SVMs. When there is a class imbalance, SVMs are biased toward the majority class, like many other machine learning algorithms. Cost-sensitive SVMs address the class imbalance problem by weighting the `C` parameter differently for each class. If we assign a larger weight to the minority class than the majority one, the model gets a greater penalty for misclassifying the minority class. So, more points in the minority class would be classified correctly, with the trade-off of classifying a few majority data points incorrectly.

We can implement a cost-sensitive SVM in R by using the `class.weights` parameter and specifying the weight for each class. For each SVM below, we will give the majority class ("No") a weight of 1 and the minority class ("Yes") a weight of 10. Using cost-sensitive SVMs allowed me to get a decision boundary and a more balanced accuracy.




## Define helper functions

Instead of minimizing the accuracy while tuning hyperparameters, we will minimize the F1 score. Accuracy is a misleading metric for imbalanced classes. A better metric is the F1 score because it combines precision and recall. The function below calculates the F1 score given vectors of actual and predicted class labels. It will be used for tuning.

```{r}
library(caret)

f1.func <- function(actual, predicted){
  cm <- confusionMatrix(predicted, actual, positive = "Yes", mode = "everything") # make a confusion matrix with "Yes" as the positive class
  f1.score <- cm$byClass[7]
  
  # If the F1 score returns NA, set it to 1
  if(is.na(f1.score)){
    f1.score <- 1
  }
  
  1 - f1.score              # Return 1 - F1 to minimize
}
```


```{r}
# Returns all the error metrics for evaluating a model. 
# These are:
#   1. Overall training accuracy
#   2. Training accuracy for the majority class
#   3. Training accuracy for the minority class
#   1. Overall test accuracy
#   2. Test accuracy for the majority class
#   3. Test accuracy for the minority class

# Parameters:
#   model: the model you're using for predictions
#   train.data: the training data you used for this model
#   test.data: the test data to predict
#   target.col: the name of the column for the target in the data

get.error.metrics <- function(model, train.data, test.data, target.col){
  
  pred.train <- predict(model, train.data) # predicted classes on the training data
  actual.train <- train.data[, target.col]
  
  train.cm <- confusionMatrix(pred.train, actual.train, mode = "everything", positive = "Yes")
  train.table <- train.cm$table # just the table
  
  train.acc <- as.numeric(train.cm$overall["Accuracy"]) # overall training accuracy
  train.majority.acc <- train.table["No", "No"]/(train.table["No", "No"] + train.table["Yes", "No"])    # acc for majority class for training
  train.minority.acc <- train.table["Yes", "Yes"]/(train.table["No", "Yes"] + train.table["Yes", "Yes"]) # acc for minority class for training

  
  pred.test <- predict(model, test.data)
  actual.test <- test.data[, target.col]
  
  test.cm <- confusionMatrix(pred.test, actual.test, mode = "everything", positive = "Yes")
  test.table <- test.cm$table # just the table
  
  test.acc <- as.numeric(test.cm$overall["Accuracy"])                                              # overall test acc
  test.majority.acc <- test.table["No", "No"]/(test.table["No", "No"] + test.table["Yes", "No"]) # test acc for majority class
  test.minority.acc <- test.table["Yes", "Yes"]/(test.table["No", "Yes"] + test.table["Yes", "Yes"]) # acc for minority class for training
  
  return (data.frame(TrainAcc = train.acc, TrainMajorityAcc = train.majority.acc, TrainMinorityAcc = train.minority.acc, 
               TestAcc = test.acc, TestMajorityAcc = test.majority.acc, TestMinorityAcc = test.minority.acc))
}
```

```{r}
library(ROCR)

# Function to make an ROC plot.
# Parameters:
#   fitted.values: numerical fitted values of SVM predictions
#   true.classes: actual class labels

rocplot <- function(fitted.values, true.classes, ...) {
  predob <- prediction(fitted.values, true.classes)
  perf <- performance(predob, "tpr", "fpr")
  
  plot(perf, ...)
}
```


## Use an SVM with a linear kernel to predict diabetes based on `bmi`, `weight`, and `age`

First, we will tune the cost parameter. 

```{r}
library(e1071)

# Perform tuning using 2-fold cross-validation instead of 10-fold, to decrease computational time
tuneControl = tune.control(cross = 2, error.fun = f1.func)

set.seed(1)
tune.out <- tune(svm, 
                 had_diabetes ~ ., 
                 data = diabetes.linear.train, 
                 kernel = "linear", 
                 ranges = list(cost = c(0.01, 0.1, 1, 10)),
                 scale = FALSE,
                 class.weights = c(No = 1, Yes = 10),
                 tunecontrol = tuneControl
                 )

summary(tune.out)
```

```{r}
tune.out$best.parameters
```

The best model had a cost of 0.01. A small cost means that many data points are allowed to be on the wrong side of the margin.

```{r}
svm.diab.linear <- svm(had_diabetes ~ ., 
                       data = diabetes.linear.train, 
                       kernel = "linear",
                       cost = 0.01,
                       scale = FALSE,
                       class.weights = c(No = 1, Yes = 10),
                       decision.values = TRUE)
```


```{r}
summary(svm.diab.linear)
```

Now letâ€™s get the training and test error rates.

```{r}
get.error.metrics(svm.diab.linear, diabetes.linear.train, diabetes.linear.test, "had_diabetes")
```

Given the class imbalance, this model performs pretty well. The training accuracy for the majority class is 78.7%, and the training accuracy for the minority class is 62.2%, almost as high. That's a huge improvement from 0% accuracy for the minority class. All the test accuracy metrics are about as high as the training ones, so the model did not overfit.

```{r}
plot(svm.diab.linear, diabetes.linear.train, bmi ~ age)
```

The plot above shows that older people or people with a high BMI are predicted to have diabetes. Younger people are not predicted to have diabetes unless they have a higher BMI. But, as the age increases, BMI matters less, and people with lower BMIs are predicted to have diabetes. For example, if someone's age is at the mean (`age = 0`), they need to have a BMI approximately 2 standard deviations above the mean to be predicted to have diabetes. But, if someone's age is 2 standard deviations above the mean, they can have a BMI one standard deviation *below* the mean, but still be predicted to have diabetes. So, older people are more at risk of diabetes.

```{r}
plot(svm.diab.linear, diabetes.linear.train, bmi ~ weight)
```

This plot shows that people with a high BMI are predicted to have diabetes. If people have a BMI slightly more than 2 standard deviations above the mean, they're always predicted to have diabetes. People with a BMI one standard deviation or less above the mean, are never predicted to have diabetes. Weight doesn't influence the predictions much. As the weight increases, the BMI threshold for having diabetes becomes slightly lower. The threshold goes from about 2 standard deviations above the mean to about 1 standard deviation. 

```{r}
plot(svm.diab.linear, diabetes.linear.train, weight ~ age)
```

This plot shows that older people or people who weigh more are predicted to have diabetes. Age is especially an important factor: If someone's age is at least 1.5 standard deviations above the mean (approximately), they are always predicted to have diabetes, no matter their weight. But, if someone's age is less than 0.5 standard deviations above the mean (approximately), they are never predicted to have diabetes. Weight is only important between 0.5 and 1.5 standard deviations above the mean age. In that range, as the age increases, the weight at which someone is predicted to have diabetes, decreases. Like the last plot, weight doesn't matter as much as the other predictor.


The plots show that, even when we use cost-sensitive SVMs, SVM is not a good model for this problem. The decision boundary shows that many of the data points in the "No" class are being predicted as "Yes". And the classes are so overlapped that even cost-sensitive SVM leaves a lot of minority samples classified incorrectly. So, we cannot really trust the results and interpretations of this model.

Let's make an ROC plot for the test data:

```{r}
fitted.vals.1 <- attributes(
                  predict(svm.diab.linear, newdata = diabetes.linear.test, decision.values = TRUE)
               )$decision.values

rocplot(-fitted.vals.1, diabetes.linear.test$had_diabetes, use.lines=FALSE)
abline(0, 1)
```

The ROC curve indicates that the model performs very badly. ROC curves that are close to the top left corner indicate a better performance. However, the model performs better than just random guessing. We know that because we plotted a diagonal line with slope = 1 and intercept = 0, and the ROC curve lies far above the line.

## Use an SVM with a polynomial kernel to predict diabetes based on `bmi`, `hours_worked_weekly`, `age`, and `height`

```{r}
set.seed(1)
tuneControl = tune.control(cross = 2, error.fun = f1.func) 

tune.out.poly <- tune(svm, 
                      had_diabetes ~ ., 
                      data = diabetes.nonlinear.train, kernel = "polynomial", 
                      ranges = list(cost = c(0.01, 0.1, 1, 10),
                                    degree = c(2, 3, 4), 
                                    coef0 = c(0, 1, 2)),
                      class.weights = c(No = 1, Yes = 10), 
                      scale = FALSE, 
                      tunecontrol = tuneControl)

summary(tune.out.poly)
```

```{r}
tune.out.poly$best.parameters
```

```{r}
svm.diab.nonlinear <- svm(had_diabetes ~ ., 
                      data = diabetes.nonlinear.train, kernel = "polynomial", 
                      cost = 0.01,
                      degree = 2,
                      coef0 = 2,
                      class.weights = c(No = 1, Yes = 10), 
                      scale = FALSE,
                      decision.values = TRUE)
```


```{r}
summary(svm.diab.nonlinear)
```
The best model again has a cost of 0.01, the lowest value. So, more data points are allowed to be on the wrong side of the margin. It has a polynomial degree of 2 and a `coef.0` value of 2.

Let's get the training and test error rates:

```{r}
get.error.metrics(svm.diab.nonlinear, diabetes.nonlinear.train, diabetes.nonlinear.test, "had_diabetes")
```

For both the training and the test data, the polynomial model performs better than the linear one for the majority class, but worse for the minority class. The polynomial model also has a better overall accuracy.

Whether this model is "better" or "worse" depends on our goals. It's better at classifying people who *do not* have diabetes, but it's worse at classifying people who *do*.

```{r}
plot(svm.diab.nonlinear, diabetes.nonlinear.train, bmi ~ age)
```

BMI and age are both important predictors. Young people (age 1 or more standard deviations below the mean) are never predicted to have diabetes. But, as the age increases, the BMI at which a person is predicted to have diabetes becomes lower and lower. At the mean age, people with a BMI above 2 standard deviations are predicted to have diabetes. But, at the rightmost side of the plot, people with a below-average BMI are now predicted to have diabetes. So, older people are more at risk of diabetes, as well as young people with a higher BMI.

```{r}
plot(svm.diab.nonlinear, diabetes.nonlinear.train, bmi ~ hours_worked_weekly)
```

Here, BMI is an important predictor, but `hours_worked_weekly` is not as important. People with a BMI about 1.5 or 2 standard deviations above the mean are always predicted to have diabetes.

```{r}
plot(svm.diab.nonlinear, diabetes.nonlinear.train, bmi ~ height)
```

Again, BMI is important, but not height. The decision boundary is almost constant as `height` changes. People with a BMI over 1.5 standard deviations above the mean (approximately) are predicted to have diabetes. 

```{r}
plot(svm.diab.nonlinear, diabetes.nonlinear.train, hours_worked_weekly ~ age)
```

Age is more important than hours worked here. Again, older people are predicted to have diabetes, while young people are not. However, at approximately `age = 1.5`, we see that people whose working hours are significantly above average (2.5+ standard deviations above) are not predicted to have diabetes. People who worked fewer hours than that, are predicted to have diabetes. 

```{r}
plot(svm.diab.nonlinear, diabetes.nonlinear.train, height ~ age)
```

Older people are predicted to have diabetes, while young people are not. Height is also important. In the region between age = 1 and slightly above that, taller people are predicted to have diabetes, while shorter people are not. That height difference decreases in that small region, so that people at shorter and shorter heights are predicted to have diabetes. When we get past `age = 1.5`, everyone is predicted to have diabetes.

```{r}
fitted.vals.2 <- attributes(
                  predict(svm.diab.nonlinear, newdata = diabetes.nonlinear.test, decision.values = TRUE)
               )$decision.values

rocplot(-fitted.vals.2, diabetes.nonlinear.test$had_diabetes, use.lines= FALSE)
abline(0, 1)
```

The ROC curve looks about the same as for the linear model. So, based only on the ROC curve, both models perform about equally well.

## Use an SVM with a radial kernel to predict the presence of cancer

```{r}
library(e1071)

set.seed(1)

tuneControl = tune.control(cross = 2, error.fun = f1.func) 
weights <- c(No = 1, Yes = 13.55)

tune.out.radial <- tune(svm, had_cancer ~ ., data = cancer.train,
                        kernel = "radial",
                        ranges = list(
                          cost = c(0.01, 0.1, 1),
                          gamma = c(0.5, 1, 2)
                        ),
                        class.weights = weights,
                        tunecontrol = tuneControl
                        )

summary(tune.out.radial)
```

```{r}
tune.out.radial$best.parameters
```

THe best cost is low, which means more data points can be in the margin. The best gamma is high, indicating a small sphere of influence for the radial kernel.

```{r}
svm.cancer <- svm(had_cancer ~ ., 
                  data = cancer.train, 
                  kernel = "radial", 
                  cost = tune.out.radial$best.parameters$cost,
                  gamma = tune.out.radial$best.parameters$gamma, 
                  class.weights = weights,
                  scale = F,
                  decision.values = T
                  )
summary(svm.cancer)
```

```{r}
get.error.metrics(svm.cancer, cancer.train, cancer.test, "had_cancer")
```

Using trial and error, I tried many different weight combinations for `class.weights`, and this one gave the most reasonable results, given all the trade-offs. It does overfit the minority class because the test accuracy for that class is about 15% lower than the training accuracy. However, other weights caused worse overfitting, 0% accuracy for one class, or unrealistic decision boundaries. 

This model predicts the minority class the best out of all the models, but it predicts the majority class the worst. So, if the goal is to accurately predict when someone *does* have a certain health condition, this model performs the best. If the goal is to predict when someone *does not* have the condition, it performs the worst. However, overfitting is a major problem, so we should take this model with a grain of salt. The fact that there were no class weights that made the model perform reasonably, suggests that a radial kernel is not the right kernel for this data.

```{r}
plot(svm.cancer, cancer.train, age ~ hours_worked_weekly, symbolPalette = c("lightblue", "black"))
```

Hours worked weekly is a strong predictor. Strangely, people whose working hours were about 2 to 3 standard deviations (SDs) above the mean, were not predicted to have cancer, no matter their age. Also, as the hours go from -3 to 2 SDs from the mean, the age at which people have cancer increases. So, working *fewer* hours *increases* the risk of cancer. This is the opposite of what I expected. I've heard that stress puts people more at risk of health conditions, so I thought that people who work *more* than an average person, would be more likely to have a health condition. The opposite is true. 

```{r}
plot(svm.cancer, cancer.train, age ~ height, symbolPalette = c("lightblue", "black"))
```

Here, we have a shape that's like a parabola because it "peaks" at height = 1. But the "peak" is small compared to the trends we've seen with other predictors. For people whose height is about 1 SD above the mean, they have a higher age at which they're predicted to have cancer. People at different heights are predicted to have cancer at lower ages. I didn't think there was a correlation between height and cancer. However, I can't fully trust this model because of overfitting, class imbalance, and class overlap. 

Height = 1 -> less risk of cancer

```{r}
library(scales)
plot(svm.cancer, cancer.train, age ~ weight, symbolPalette = c("lightblue", "green"))
```

People with a higher weight (> 2 SDs above the mean) are never predicted to have cancer. Before that, as the weight increases, the age at which someone is predicted to have cancer increases. So, if someone has a lower weight, they're more likely to have cancer at a younger age.

Weight > 2 SDs above mean => no risk of cancer

Before that, lower weight = risk of cancer at young age

```{r}
# Plotting the test data here because I couldn't see the boundary with the training data
plot(svm.cancer, cancer.test, age ~ bmi, symbolPalette = c("lightblue", "green"))
```
People with a higher BMI (> 2.5 SDs) above the mean are never predicted to have cancer, regardless of age.

```{r}
plot(svm.cancer, cancer.train, age ~ days_used_alcohol_yearly, symbolPalette = c("lightblue", "green"))
```

Both predictors are important. We have a parabolic-looking trend where people whose alcohol use was below the mean or 3 SDs above are predicted to have cancer at younger ages. But then, people whose alcohol use is 1 or 2 SDs above the mean need to be older in order to have cancer. This is surprising because I've heard that high alcohol use increases the risk of cancer. But, here, people with alcohol use *below the mean* or 3 standard deviations above the mean are predicted to have cancer at younger ages.

Alcohol use = 1 => less likely to have cancer

Alcohol use != 1 => more likely

Let's plot the ROC curve and compare it with the others:

```{r}

```

```{r}
fitted.vals.3 <- attributes(
                  predict(svm.cancer, newdata = cancer.test, decision.values = TRUE)
               )$decision.values

rocplot(-fitted.vals.1, diabetes.linear.test$had_diabetes, col="red", use.lines = FALSE)

rocplot(-fitted.vals.2, diabetes.nonlinear.test$had_diabetes, col="blue", add = T)

rocplot(-fitted.vals.3, cancer.test$had_cancer, col="green", add = T)
abline(0, 1)

legend("bottomright", legend = c("Linear Kernel - Diabetes", "Polynomial Kernel - Diabetes", "Radial Kernel - Cancer"), fill = c("red", "blue", "green"))
title("ROC Curves for All Models")
```

The linear model and the polynomial one are about equally close to the top left corner. The radial model is farther from the top left corner and closer to the diagonal line. So, the overall performance of the linear and polynomial models, considering all trade-offs, is better than the radial one.














